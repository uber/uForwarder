service:
  name: ${UDEPLOY_SERVICE_NAME:kafka-consumer-proxy}

logging:
  pattern:
    level: "%X"

# GRPC Server configurations
grpc:
  # Port for GRPC Server
  port: 0

server:
  # Port for jetty
  port: 0

system:
  port: 0

# These are the configurations for the system debug pages.
management:
  # These configurations manage the jobs.html debug pages
  job:
    enabled: true
  # These configurations manage the workers.html debug pages
  worker:
    enabled: true
    # workerUdg sets the udg path for the worker to link to worker debug page from master debug page.
    workerUdg: "udg://kafka-consumer-proxy-worker"

# These are the configurations for the master
master:
  rebalancer:
    mode: default
    numWorkersPerUri: 1
  autoscalar:
    enabled: true
    sampleInterval: 5000
  jobCreator: default
  # These are configurations that control Kafka related operations
  kafka:
    # Determines the frequency of checking kafka topic partition expansion or shrink.
    # When partition expansion or shrink happens, the master will create new jobs or destroy old
    # jobs.
    partition.expansion.watcher.interval: 60000 # 1 min in ms
    # These are the configurations for the Kafka AdminClient.
    admin:
      # This determines how we will resolve the kafka brokers.
      resolverClass: com.uber.data.kafka.datatransfer.common.StreamingCommonKafkaClusterResolver
      # According to Kafka, the client ID should be unique in a Kafka consumer proxy instance.
      clientId: kafka-consumer-proxy-admin-test
      # This is the bootstrap server that is used if resovlerMode = STATIC.
      bootstrapServers: 127.0.0.1:9093
      # This determines whether or not to load secure configuration for kafka admin client
      enableSecure: true
    # These are the configurations for the KafkaConsumer used to commit offsets for all
    # topic-partitions.
    offsetCommitter:
      # This determines how we will resolve the kafka brokers.
      resolverClass: com.uber.data.kafka.datatransfer.common.StreamingCommonKafkaClusterResolver
      # This is the bootstrap server that is used if resovlerMode = STATIC.
      bootstrapServers: 127.0.0.1:9093
  # These are configurations that control the managers.
  # Managers are responsible for maintain the state of the system, but defer persistent to the store.
  manager:
    job:
      # Determines the frequency of emitting metrics on job state.
      metricsInterval: 60000 # 1 min in ms
      # Determines the frequency that the rebalance loop is run.
      # Reducing this value means that system will detect unassigned jobs faster
      # but at the cost of additional store calls.
      rebalanceInterval: 10000 # 10s in ms
    worker:
      # Determines the frequency of emitting metrics on worker state.
      metricsInterval: 60000 # 1 min in ms
  # These configurations control the store.
  store:
    mode: ZK
    worker:
      # This sets the worker TTL.
      # If a worker misses heartbeats for this amount of time, the work will be reassigned to another worker.
      ttl: 2m
    jobStatus:
      # This sets the interval for async/batched writes to underlying storage.
      # Internally, this class maintains an in-memory WAL that is periodically written to ZK in a batch.
      asyncWriteInterval: 10s
      # This sets the job status TTL.
      ttl: 5m
  zookeeper:
    # This sets the zkConnection
    # coordinator uses zkConnection for leader election
    # If store.mode is ZK, it will read connection string from zkConnection
    zkConnection: 127.0.0.1:2181/kafka-consumer-proxy
  coordinator:
    leaderSelector:
      # Determines the frequency of emitting metrics on leader selector state.
      metricsInterval: 60000 # 1 min in ms

# These are configurations for the worker.
worker:
  # These are configurations for the pipeline
  pipeline:
    manager:
      # This is the interval for the pipeline manager to garbage collect unused pipelines
      gcInterval: 60000 # 1 min in ms
      # This is the interval for the pipeline manager to log and report metrics
      metricsInterval: 60000 # 1 min in ms
  # These are configurations for the controller
  controller:
    type: default
    grpc:
      # This is the host:port that the worker will use to contact the master.
      masterHostPort: 127.0.0.1:8087
      # This is the frequency of heartbeat requests are sent from the worker to the master.
      heartbeatInterval: 10s
      # This is the jitter applied to the heartbeats.
      heartbeatJitter: 1s
      # This is the timeout for the heartbeats.
      heartbeatTimeout: 10s
      # This is the worker lease.
      # If a worker does not successfully heartbeat within the worker lease window,
      # it will assume that it no longer has a valid lease and cancelAll jobs locally.
      workerLease: 1m
  # These are configurations for the dispatchers.
  dispatcher:
    # These are configurations for grpc dispatchers.
    authorizer:
      kafkaCharterClientConfigs:
        # This sets the environment of charter that we will hit. Setting it to false will hit Staging.
        authorizer.charter.is.prod: false
        # This is used to configure the refresh time of the cache in minutes
        authorizer.charter.cache.refresh.min: 5
        # This is used to decide the charter domain that we are looking for permissions in
        authorizer.charter.policy.domain: test.topics.kafka
        # Flag to enable/disable caching in charter authorizer
        authorizer.charter.cache.enable: true
        # Fallback to UCS if Charter Backend is not available
        authorizer.charter.is.ucs.enabled: true
        # Lookup Charter via UCS (instead of RPC)? Default: True
        authorizer.charter.ucs.lookup.first: true
        # Which UCS repo to read the policies from
        authorizer.charter.ucs.repo: charter-policy@topics.kafka
        # This decides if authorization failures will actually be returned as false. If this is set to
        # true, then unauthorized request will still go through but with a metric being emitted.
        allow.fail.open: false
      # This is used to form the resource string used for authorization check. If set to staging then
      # we still still hit charter production but only look for "staging" permissions
      environment: staging
    grpc:
      # This sets the minimum RPC timeout value so that we don't automatically timeout on all rpc calls.
      minRpcTimeoutMs: 2
      # This sets the maximum RPC timeout value so that we don't automatically block for rpc calls.
      maxRpcTimeoutMs: 1800001 # 30 mins + 1ms
      # This is the number of gRPC channels in the gRPC channel pool.
      grpcChannelPoolSize: 2
    # These are configurations for kafka retryQ/DLQ dispatchers.
    kafka:
      enabled: true
      # This determines how we will resolve the kafka brokers.
      resolverClass: com.uber.data.kafka.datatransfer.common.KafkaClusterResolver
      # This is the bootstrap server that is used if resovlerMode = STATIC.
      bootstrapServer: 127.0.0.1:9092
      # The acks setting that we pass to Apache Kafka Producer.
      acks: all
  # These are the configurations for the processors.
  processor:
    # The thread pool size for each processor
    threadPoolSize: 2
    # The max number of unprocessed inbound messages cached for each topic-partition.
    maxInboundCacheCount: 1001
    # The max number of unprocessed inbound message cached for the processor
    maxProcessorInBoundCacheCount: 9999
    # The max number of unprocessed outbound messages cached for each topic-partition.
    # This number can be override by user-defined JobConfiguration.
    maxOutboundCacheCount: 251
    # The max difference between an ACKed offset and the largest committed offset for each
    # topic-partition.
    maxAckCommitSkew: 10001
    # This filters out messages that was not originally produced to the cluster that it is being read from.
    clusterFilterEnabled: true
    experimentalLimiterEnabled: true
    regionalFilterEnabled: true
    useLinkedListAckTrackingQueue: true
  # These are the configurations for the fetchers.
  fetcher:
    # These are the configurations for the kafka fetcher.
    kafka:
      # Enable the kafka spring auto configuration.
      # Setting this to true will automatically inject the kafka fetcher into the spring container.
      enabled: true
      # This determines how we will resolve the kafka brokers.
      resolverClass: com.uber.data.kafka.datatransfer.common.KafkaClusterResolver
      # This is the bootstrap server that is used if resolverClass = STATIC.
      bootstrapServer: 127.0.0.1:9092

# TODO (haitao.zhang): fix pipeline factory auto configuration settings
datatransfer:
  worker:
    pipeline:
      factory:
        autoconfiguration:
          enabled: false # disable default pipeline factory
      manager:
        gcInterval: 60000 # 1 min in ms
        metricsInterval: 60000 # 1 min in ms
